{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY_Jc3MmWynW",
        "outputId": "07533f28-4369-4f13-ea74-fc1107cb9ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfJuU76xW5J4"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci8VelWoW7og",
        "outputId": "e12c1c31-fcb3-45da-a685-3c78d31d0a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTJ7OjylWxzP"
      },
      "outputs": [],
      "source": [
        "# You need to define a config file in the form of dictionary or yaml\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'sweep cross entropy',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "         'hidden_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        },\n",
        "        'loss': {\n",
        "            'values': ['cross_entropy','squared_loss']\n",
        "        },\n",
        "\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_TA(Ass_1)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "B6bflD5lV5HY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.datasets import fashion_mnist\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5u4moovWSQv",
        "outputId": "691b446c-aa74-49f6-9bd3-6e351fcb5621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(48000, 28, 28)\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "(48000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST data using Keras\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(x_train.shape)\n",
        "x_train,x_val, y_train, y_val=train_test_split(x_train,y_train, test_size=0.2,shuffle=True,random_state=42)\n",
        "print(x_train.shape)\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float128') / 255.0\n",
        "x_val = x_val.astype('float128') / 255.0\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Fit and transform the target values using OneHotEncoder\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "print(y_train[0])\n",
        "y_val = encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape((-1, 28 * 28))\n",
        "print(x_train.shape)\n",
        "x_val = x_val.reshape((-1, 28 * 28))\n",
        "x_test = x_test.reshape((-1, 28 * 28))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aa7hKTVx_cwg"
      },
      "outputs": [],
      "source": [
        "class Activations :\n",
        "  def sigmoid(self,x) :\n",
        "    if(x>200):\n",
        "      return 1\n",
        "    if(x<-200):\n",
        "      return 0\n",
        "    else :\n",
        "      return 1/(1 + math.exp(-x))\n",
        "\n",
        "  def tanh(self,x) :\n",
        "      t1 = math.exp(x)\n",
        "      t2  = math.exp(-x)\n",
        "      return (t1-t2)/(t1 + t2)\n",
        "\n",
        "  def g1(self,a):\n",
        "    n = len(a)\n",
        "    h = np.array(a)\n",
        "    for i in range(n) :\n",
        "      h[i] = self.sigmoid(a[i])\n",
        "    return h\n",
        "\n",
        "  def g2(self,a):\n",
        "    n = len(a)\n",
        "    h = np.array(a)\n",
        "    for i in range(n) :\n",
        "      h[i] = self.tanh(a[i])\n",
        "    return h\n",
        "\n",
        "  def g3(self,a):\n",
        "    n = len(a)\n",
        "    h = np.array(a)\n",
        "    for i in range(n) :\n",
        "      h[i] = max(0,a[i])\n",
        "    return h\n",
        "\n",
        "  def SoftMax(self,a):\n",
        "    max_a = np.max(a)\n",
        "    exp_a = np.exp(a - max_a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    return y\n",
        "\n",
        "  '''def SoftMax(self,a) :\n",
        "    erx = np.array(a)\n",
        "    sum = 0\n",
        "    for i in range(len(a)):\n",
        "      erx[i] = math.exp(a[i])\n",
        "      sum = sum + erx[i]\n",
        "    y = np.array(a)\n",
        "    for i in range(len(a)):\n",
        "      y[i] = erx[i]/sum\n",
        "    return y'''\n",
        "\n",
        "\n",
        "class Differential :\n",
        "  def sig_dif(self,a):\n",
        "    n = len(a)\n",
        "    g_dash = np.empty(n)\n",
        "    activ = Activations()\n",
        "    for i in range(n) :\n",
        "      g_x = activ.sigmoid(a[i])\n",
        "      g_dash[i] = g_x*(1-g_x)\n",
        "    return g_dash\n",
        "\n",
        "  def tan_dif(self,a):\n",
        "    n = len(a)\n",
        "    g_dash = np.empty(n)\n",
        "    activ = Activations()\n",
        "    for i in range(n) :\n",
        "      f_x = activ.tanh(a[i])\n",
        "      g_dash[i] = 1 - (f_x ** 2)\n",
        "    return g_dash\n",
        "\n",
        "  def Rel_dif(self,a):\n",
        "    n = len(a)\n",
        "    h = np.empty(n)\n",
        "    for i in range(n) :\n",
        "      if a[i]>=0 :\n",
        "        h[i] = 1\n",
        "      else :\n",
        "        h[i] = 0\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "arMrOZ2KdNvj"
      },
      "outputs": [],
      "source": [
        "class Initializer :\n",
        "\n",
        "  def Initialize(self,hidden_layers,npl):\n",
        "    self.W = [[]]    # list consisting of all the W's\n",
        "\n",
        "    # Create and append each Wi matrix filled with zeros to the list\n",
        "    self.W.append(np.zeros((npl,784),dtype=np.float128))         # input layer h0\n",
        "\n",
        "    for _ in range(hidden_layers-1):\n",
        "        self.W.append(np.zeros((npl,npl),dtype=np.float128))\n",
        "\n",
        "    self.W.append(np.zeros((10,npl),dtype=np.float128))\n",
        "\n",
        "    self.b = [[]]    # list consiting of all the b's\n",
        "\n",
        "    # Create and append each bi vector filled with zeros to the list\n",
        "    self.b.append(np.zeros(npl,dtype=np.float128))\n",
        "\n",
        "    for _ in range(hidden_layers-1):\n",
        "        self.b.append(np.zeros(npl,dtype=np.float128))\n",
        "\n",
        "    self.b.append(np.zeros(10,dtype=np.float128))\n",
        "\n",
        "    return self.W,self.b\n",
        "\n",
        "\n",
        "  def Initialize2(self,hidden_layers,npl):\n",
        "      self.W = [[]]    # list consisting of all the W's\n",
        "\n",
        "      # Create and append each Wi matrix filled with random values to the list\n",
        "      self.W.append(np.random.randn(npl, 784).astype(np.float128))         # input layer h0\n",
        "\n",
        "      for _ in range(hidden_layers-1):\n",
        "          self.W.append(np.random.randn(npl, npl).astype(np.float128))\n",
        "\n",
        "      self.W.append(np.random.randn(10, npl).astype(np.float128))\n",
        "\n",
        "      self.b = [[]]    # list consiting of all the b's\n",
        "\n",
        "      # Create and append each bi vector filled with random values to the list\n",
        "      self.b.append(np.random.randn(npl).astype(np.float128))\n",
        "\n",
        "      for _ in range(hidden_layers-1):\n",
        "          self.b.append(np.random.randn(npl).astype(np.float128))\n",
        "\n",
        "      self.b.append(np.random.randn(10).astype(np.float128))\n",
        "      return self.W,self.b\n",
        "\n",
        "  def XavierIntializer(self,hidden_layers,npl):\n",
        "    self.W = [[]]\n",
        "    self.b = [[]]\n",
        "    return W,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mXsbSnVAEiS9"
      },
      "outputs": [],
      "source": [
        "class Arithmetic :\n",
        "  def Add(self,u,v):\n",
        "    for i in range(1,len(u)):\n",
        "      u[i] = u[i] + v[i]\n",
        "    return u\n",
        "\n",
        "  def Subtract(self,v,dv,eta):\n",
        "    for i in range(1,len(v)):\n",
        "      v[i] = v[i] - (eta * dv[i])\n",
        "    return v\n",
        "\n",
        "  def RMSpropSubtract(self,v,dv,lv,eps,eta):\n",
        "    for i in range(1,len(v)):\n",
        "      ueta = eta/(np.sqrt(np.sum(lv[i])) + eps)\n",
        "      v[i] = v[i] - (ueta * dv[i])\n",
        "    return v\n",
        "\n",
        "  def AdamSubtract(self,V,mV_hat,vV_hat,eps,eta):\n",
        "    for i in range(1,len(V)):\n",
        "      norm = np.linalg.norm(vV_hat[i])\n",
        "      ueta = eta/(np.sqrt(norm) + eps)\n",
        "      V[i] = V[i] - (ueta * mV_hat[i])\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "N4kr8oNeXJ5K"
      },
      "outputs": [],
      "source": [
        "class Gradient_descent :\n",
        "  def __init__(self, input_size, output_size, config):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.layers = config['layers']\n",
        "    self.activation = config['activation']\n",
        "    self.npl = config[\"neurons_per_layer\"]\n",
        "    self.eta = config[\"learning_rate\"]\n",
        "    self.batch = config[\"batch_size\"]\n",
        "    self.init = config[\"Initialization\"]\n",
        "    self.config = config\n",
        "\n",
        "  def backward_propagation(self,A,H,W,b,y):\n",
        "    L = self.layers\n",
        "\n",
        "    delA = [[]]*(L+1)\n",
        "    delW = [[]]*(L+1)\n",
        "    delb = [[]]*(L+1)\n",
        "    delh = [[]]*(L+1)\n",
        "\n",
        "    delA[L] = -(y - H[L])\n",
        "\n",
        "    for k in range(L,0,-1):\n",
        "      delW[k] = np.outer(delA[k],H[k-1])\n",
        "      delb[k] = delA[k]\n",
        "      delh[k-1] = W[k].T @ delA[k]\n",
        "      if k>1 :\n",
        "        diff_vect = np.array(A[k-1])\n",
        "        d = Differential()\n",
        "        if self.activation == 'sigmoid':\n",
        "          diff_vect = d.sig_dif(A[k-1])\n",
        "        elif self.activation == 'tanh':\n",
        "          diff_vect = d.tan_dif(A[k-1])\n",
        "        else :\n",
        "          diff_vect = d.Rel_dif(A[k-1])\n",
        "        delA[k-1] = np.multiply(delh[k-1],diff_vect)\n",
        "\n",
        "    return delW,delb\n",
        "\n",
        "  def forward_propagation(self,W,b,layers,inpl):\n",
        "    A = [[]]\n",
        "    H = [inpl]\n",
        "    activ = Activations()\n",
        "\n",
        "    for i in range (1,layers) :\n",
        "      a = b[i] + (W[i] @ H[i-1])\n",
        "\n",
        "      A.append(a)\n",
        "      h = np.array(a)\n",
        "\n",
        "      if self.activation == 'sigmoid':   # Sigmoid activation function\n",
        "        h = activ.g1(a)\n",
        "      elif self.activation == 'tanh':   # tanh activation function\n",
        "        h = activ.g2(a)\n",
        "      else :            # ReLU activation function\n",
        "        h = activ.g3(a)\n",
        "\n",
        "      H.append(h)\n",
        "\n",
        "    a = b[layers] + np.inner(W[layers],H[layers-1])\n",
        "    A.append(a)\n",
        "    y_hat = activ.SoftMax(a)\n",
        "    H.append(y_hat)\n",
        "    #print('y_hat = ',y_hat)\n",
        "    return A,H\n",
        "\n",
        "  def Stocastic_Gradient_descent(self) :\n",
        "    epochs = self.config['epochs']\n",
        "    W = []    # list consisting of all the W's\n",
        "    b = []    # list consiting of all the b's\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "\n",
        "    training_loss = []\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "      for j in range(len(x_train)): #len(trainX)\n",
        "\n",
        "        h0 = x_train[j]\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "\n",
        "        tdelW,tdelb = self.backward_propagation(A,H,W,b,y)\n",
        "        PMA = Arithmetic()    # P - Plus, M - Minus , A - Arithmetic\n",
        "        delW = PMA.Add(delW,tdelW)\n",
        "        delb = PMA.Add(delb,tdelb)\n",
        "        if((j+1) % self.batch == 0):\n",
        "          W = PMA.Subtract(W,delW,self.eta)\n",
        "          b = PMA.Subtract(b,delb,self.eta)\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def Momentum_Gradient_descent(self) :\n",
        "    epochs = self.config['epochs']\n",
        "    W = []    # list consisting of all the W's\n",
        "    b = []    # list consiting of all the b's\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "\n",
        "    prev_uW,prev_ub = I.Initialize(self.layers-1,self.npl)\n",
        "    beta = 0.9\n",
        "\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "      for j in range(len(x_train)):\n",
        "\n",
        "        h0 = x_train[j]\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "\n",
        "        temp_delW,temp_delb = self.backward_propagation(A,H,W,b,y)\n",
        "        PMA = Arithmetic()\n",
        "        delW = PMA.Add(delW,temp_delW)\n",
        "        delb = PMA.Add(delb,temp_delb)\n",
        "\n",
        "        if((j+1)%self.batch == 0):\n",
        "          uW = [[]]\n",
        "          ub = [[]]\n",
        "          for k in range(1,len(prev_uW)):\n",
        "            uW.append(beta*prev_uW[k] + delW[k])\n",
        "            ub.append(beta*prev_ub[k] + delb[k])\n",
        "\n",
        "          W = PMA.Subtract(W,uW,self.eta)\n",
        "          b = PMA.Subtract(b,ub,self.eta)\n",
        "          prev_ub = ub\n",
        "          prev_uW = uW\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def NAG_descent(self) :    # function(layers ,neurons per layer)\n",
        "    epochs =  self.config['epochs']\n",
        "    W = []    # list consisting of all the W's\n",
        "    b = []    # list consiting of all the b's\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "    prev_vW,prev_vb = I.Initialize(self.layers-1,self.npl)\n",
        "    beta = 0.9\n",
        "\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "      vW = [[]]\n",
        "      vb = [[]]\n",
        "      for k in range(1,len(prev_vW)):\n",
        "          vW.append(beta*prev_vW[k])\n",
        "          vb.append(beta*prev_vb[k])\n",
        "\n",
        "      for j in range(len(x_train)):\n",
        "        h0 = x_train[j]\n",
        "        #print(h0,len(h0))\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "\n",
        "        ASA = Arithmetic()\n",
        "        tempW = ASA.Subtract(W,vW,beta)\n",
        "        tempb = ASA.Subtract(b,vb,beta)\n",
        "\n",
        "        temp_delW,temp_delb = self.backward_propagation(A,H,tempW,tempb,y)\n",
        "        delW = ASA.Add(delW,temp_delW)\n",
        "        delb = ASA.Add(delb,temp_delb)\n",
        "\n",
        "        if((j+1)%self.batch == 0):\n",
        "          for k in range(len(prev_vW)):\n",
        "            vW[k] += delW[k]\n",
        "            vb[k] += delb[k]\n",
        "\n",
        "          W = ASA.Subtract(W,vW,self.eta)\n",
        "          b = ASA.Subtract(b,vb,self.eta)\n",
        "          prev_vb = vb\n",
        "          prev_vW = vW\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def RMSprop(self) :\n",
        "    epochs = self.config[\"epochs\"]\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "\n",
        "    vW,vb = I.Initialize(self.layers-1,self.npl)\n",
        "    beta = 0.9\n",
        "    eps = 1e-4\n",
        "    batch_size = 50\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "      for j in range(len(x_train)): #len(trainX)\n",
        "        h0 = x_train[j]\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "        temp_delW,temp_delb = self.backward_propagation(A,H,W,b,y)\n",
        "        PMA = Arithmetic()\n",
        "        delW = PMA.Add(delW,temp_delW)\n",
        "        delb = PMA.Add(delb,temp_delb)\n",
        "\n",
        "        if((j+1)% self.batch == 0):\n",
        "          for k in range(1,len(vW)):\n",
        "            vW[k] = (beta * vW[k])+ ((1-beta)*(delW[k]**2))\n",
        "            vb[k] = (beta * vb[k])+ ((1-beta)*(delb[k]**2))\n",
        "\n",
        "          W = PMA.RMSpropSubtract(W,delW,vW,eps,self.eta)\n",
        "          b = PMA.RMSpropSubtract(b,delb,vb,eps,self.eta)\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def Adam(self) :\n",
        "    epochs = self.config[\"epochs\"]\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "\n",
        "    vW,vb = I.Initialize(self.layers-1,self.npl)\n",
        "    mW,mb = I.Initialize(self.layers-1,self.npl)\n",
        "    beta1,beta2 = 0.9,0.999\n",
        "\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "      eps = 1e-10\n",
        "\n",
        "      for j in range(len(x_train)): #len(trainX)\n",
        "        h0 = x_train[j]\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "        temp_delW,temp_delb = self.backward_propagation(A,H,W,b,y)\n",
        "        PMA = Arithmetic()\n",
        "        delW = PMA.Add(delW,temp_delW)\n",
        "        delb = PMA.Add(delb,temp_delb)\n",
        "\n",
        "\n",
        "        if((j+1)% self.batch == 0):\n",
        "          vW_hat,vb_hat = I.Initialize(self.layers-1,self.npl)\n",
        "          mW_hat,mb_hat = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "          for k in range(1,len(vW)):\n",
        "            mW[k] = (beta1 * mW[k]) + ((1-beta1)*(delW[k]))\n",
        "\n",
        "            mW_hat[k] = mW[k]/(1-np.power(beta1,i+1))\n",
        "\n",
        "            mb[k] = (beta1 * mb[k]) + ((1-beta1)*(delb[k]))\n",
        "\n",
        "            mb_hat[k] = mb[k]/(1-np.power(beta1,i+1))\n",
        "\n",
        "            vW[k] = (beta2 * vW[k])+ ((1-beta2)*(delW[k]**2))\n",
        "\n",
        "            vW_hat[k] = vW[k]/(1-np.power(beta2,i+1))\n",
        "\n",
        "            vb[k] = (beta2 * vb[k])+ ((1-beta2)*(delb[k]**2))\n",
        "\n",
        "            vb_hat[k] = vb[k]/(1-np.power(beta2,i+1))\n",
        "\n",
        "          W = PMA.AdamSubtract(W,mW_hat,vW_hat,eps,self.eta)\n",
        "          b = PMA.AdamSubtract(b,mb_hat,vb_hat,eps,self.eta)\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def NAdam(self) :\n",
        "    epochs = self.config[\"epochs\"]\n",
        "\n",
        "    I = Initializer()\n",
        "    if(self.init == \"random\"):\n",
        "      W,b = I.Initialize2(self.layers-1,self.npl)\n",
        "    else:\n",
        "      W,b = W,b = I.XavierIntializer(self.layers-1,self.npl)\n",
        "\n",
        "    vW,vb = I.Initialize(self.layers-1,self.npl)\n",
        "    mW,mb = I.Initialize(self.layers-1,self.npl)\n",
        "    beta1,beta2 = 0.9,0.999\n",
        "\n",
        "    for i in range(epochs):\n",
        "      delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "      eps = 1e-10\n",
        "\n",
        "      for j in range(len(x_train)): #len(trainX)\n",
        "        h0 = x_train[j]\n",
        "        A,H = self.forward_propagation(W,b,self.layers,h0)\n",
        "        y = y_train[j]\n",
        "        temp_delW,temp_delb = self.backward_propagation(A,H,W,b,y)\n",
        "        PMA = Arithmetic()\n",
        "        delW = PMA.Add(delW,temp_delW)\n",
        "        delb = PMA.Add(delb,temp_delb)\n",
        "\n",
        "\n",
        "        if((j+1)% self.batch == 0):\n",
        "          vW_hat,vb_hat = I.Initialize(self.layers-1,self.npl)\n",
        "          mW_hat,mb_hat = I.Initialize(self.layers-1,self.npl)\n",
        "          uW_hat,ub_hat = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "          for k in range(1,len(vW)):\n",
        "            mW[k] = (beta1 * mW[k]) + ((1-beta1)*(delW[k]))\n",
        "\n",
        "            mW_hat[k] = mW[k]/(1-np.power(beta1,i+1))\n",
        "\n",
        "            mb[k] = (beta1 * mb[k]) + ((1-beta1)*(delb[k]))\n",
        "\n",
        "            mb_hat[k] = mb[k]/(1-np.power(beta1,i+1))\n",
        "\n",
        "            vW[k] = (beta2 * vW[k])+ ((1-beta2)*(delW[k]**2))\n",
        "\n",
        "            vW_hat[k] = vW[k]/(1-np.power(beta2,i+1))\n",
        "\n",
        "            vb[k] = (beta2 * vb[k])+ ((1-beta2)*(delb[k]**2))\n",
        "\n",
        "            vb_hat[k] = vb[k]/(1-np.power(beta2,i+1))\n",
        "\n",
        "            uW_hat[k] = (beta1*mW_hat[k]) + (((1-beta1)/(1-(beta1)**(i+1)))*delW[k])\n",
        "\n",
        "            ub_hat[k] = (beta1*mb_hat[k]) + (((1-beta1)/(1-(beta1)**(i+1)))*delb[k])\n",
        "\n",
        "\n",
        "          W = PMA.AdamSubtract(W,uW_hat,vW_hat,eps,self.eta)\n",
        "          b = PMA.AdamSubtract(b,ub_hat,vb_hat,eps,self.eta)\n",
        "          delW,delb = I.Initialize(self.layers-1,self.npl)\n",
        "\n",
        "    return W,b\n",
        "\n",
        "  def predict_class(self,W,b,layers,inpl,y):\n",
        "    A = [[]]\n",
        "    H = [inpl]\n",
        "    activ = Activations()\n",
        "\n",
        "    for i in range (1,layers) :\n",
        "      innprod = np.inner(W[i],H[i-1])\n",
        "      a = b[i] + innprod\n",
        "      A.append(a)\n",
        "      h = np.array(a)\n",
        "      if self.activation == 'sigmoid':   # Sigmoid activation function\n",
        "        h = activ.g1(a)\n",
        "      elif self.activation == 'tanh':   # tanh activation function\n",
        "        h = activ.g2(a)\n",
        "      else :            # ReLU activation function\n",
        "        h = activ.g3(a)\n",
        "      H.append(h)\n",
        "\n",
        "    a = b[layers] + np.inner(W[layers],H[layers-1])\n",
        "    y_hat = activ.SoftMax(a)\n",
        "    #print(y_hat,y)\n",
        "    i1 = np.argmax(y)\n",
        "    i2 = np.argmax(y_hat)\n",
        "    #print(i1,i2)\n",
        "    if i1 == i2 :\n",
        "      return True\n",
        "    else :\n",
        "      return False\n",
        "    #print(sum(y_hat))\n",
        "\n",
        "\n",
        "  def predict(self,W,b):\n",
        "    #print(W,b)\n",
        "    count = 0\n",
        "    for i in range(len(x_test)):\n",
        "      h0 = x_test[i]\n",
        "      y = y_test[i]\n",
        "      if(self.predict_class(W,b,self.layers,h0,y)):\n",
        "        count = count + 1\n",
        "\n",
        "    print(count)\n",
        "\n",
        "config = {\n",
        "    \"epochs\" : 5,\n",
        "    \"layers\": 3,\n",
        "    \"activation\": \"sigmoid\",\n",
        "    \"neurons_per_layer\" : 64,\n",
        "    \"learning_rate\" : 0.1,\n",
        "    \"batch_size\" : 1,\n",
        "    \"Initialization\" : \"random\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_-xXLsjTog9K"
      },
      "outputs": [],
      "source": [
        "Model = Gradient_descent(784,10,config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR1TsBSIojaQ",
        "outputId": "94ec3f6f-cc0b-4c73-b028-7a06ed9ec60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8093\n"
          ]
        }
      ],
      "source": [
        "W,b = Model.Adam()\n",
        "Model.predict(W,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nceTDupoheW",
        "outputId": "01a83f42-1720-45e6-b84c-d7f05cc0b724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7949\n"
          ]
        }
      ],
      "source": [
        "W,b = Model.Stocastic_Gradient_descent();\n",
        "Model.predict(W,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y239azTY_ABB",
        "outputId": "686e66f1-9953-4e90-f114-d69cfb1d5147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "W,b = Model.NAG_descent();\n",
        "Model.predict(W,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDpbePKAWrKP"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    '''\n",
        "    WandB calls main function each time with differnet combination.\n",
        "\n",
        "    We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "    '''\n",
        "\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        run_name=\"-ac_\"+wandb.config.activation+\"-hs\"+str(wandb.config.hidden_size)\n",
        "        wandb.run.name=run_name\n",
        "        #obj=NN(wandb.config['num_layers'],wandb.config['hidden_size'])\n",
        "\n",
        "        model = SimpleNN(784,10,wandb.config)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01,betas=(0.9, 0.999))\n",
        "        model.train( model,criterion,optimizer,x_train,y_train,x_val,y_val)\n",
        "\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=10) # calls main function for count number of times.\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}